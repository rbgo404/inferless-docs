---
title: "Building a Reasoning Model: Fine-tuning Qwen3-4B with GRPO"
description: "Welcome to an engaging tutorial designed to walk you through creating a customer support voicebot where users can voice their queries and receive solutions. You'll learn to integrate speech recognition, large language, and text-to-speech models to develop a responsive and efficient voice-based customer support application."
---

## Introduction

Ever wondered how to make a language model actually "think" through problems step by step? Today, we're diving into a fascinating approach that transforms Qwen3-4B-Base into a reasoning powerhouse using Group Relative Policy Optimization (GRPO).

Unlike traditional fine-tuning where models learn to mimic patterns, this method teaches models to show their work - just like how you'd solve a math problem on paper. We'll walk through the entire process, from setting up the initial model to training it with multiple reward signals that optimize for both correctness and clear reasoning.

The magic happens in two stages: first, we teach the model our reasoning format through supervised fine-tuning, then we use GRPO to make it actually good at reasoning by generating multiple attempts and learning from what works best.

In this blog post, we'll explore how to transform Qwen3-4B-Base into a powerful reasoning model using Group Relative Policy Optimization (GRPO) and OpenR1's Math dataset.

## Why This Approach Works

Traditional language models often feel like black boxes - they give you answers but you can't see the thinking process. This approach fixes that by explicitly structuring the output into reasoning and answer sections. Think of it like teaching a student to show their work in math class.

GRPO takes this further by using multiple reward functions simultaneously. Instead of just caring about the final answer, we reward the model for:
- Getting the right answer (most important)
- Following the proper format
- Using clear reasoning steps
- Maintaining consistent structure

This multi-objective approach creates models that are both accurate and transparent - a game-changer for applications where you need to trust and verify AI reasoning.

## Setting Up the Foundation

We start with Qwen3-4B-Base, a solid general-purpose model that we'll specialize for reasoning. The key is using LoRA (Low-Rank Adaptation) to efficiently fine-tune without breaking the bank on compute resources.

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096  # Room for detailed reasoning
lora_rank = 32  # Sweet spot for capability vs speed

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-4B-Base",
    max_seq_length = max_seq_length,
    load_in_4bit = False,  # 16-bit precision for math accuracy
    fast_inference = True,
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.7,
)
```

The extended sequence length gives our model room to breathe when working through complex problems. We're using 16-bit precision instead of 4-bit because mathematical reasoning benefits from the extra precision.

## Creating the Reasoning Template

Here's where we define how we want our model to think. We use a simple but effective XML-like structure:

```python
reasoning_start = "<reasoning>"
reasoning_end = "</reasoning>"
solution_start = "<answer>"
solution_end = "</answer>"

system_prompt = f"""You are given a problem.
Think about the problem and provide your reasoning.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between:
{solution_start}
...
{solution_end}"""
```

This template forces the model to separate its thinking process from its final answer. It's like requiring students to show their work - the format itself encourages better reasoning habits.

## Stage 1: Teaching the Format

Before we can optimize reasoning quality, we need to teach the model our expected format. This is where supervised fine-tuning comes in - we show the model lots of examples of properly formatted reasoning.

We start with OpenR1's GSM8K dataset, which contains high-quality mathematical reasoning traces. The preprocessing step converts these from DeepSeek's format to our custom structure:

```python
def format_dataset(x):
    expected_answer = x["answer"]
    problem = x["question"]
    
    # Clean up the existing reasoning trace
    thoughts = x["response"].replace("<think>", "").replace("</think>", "").strip()
    
    # Apply our formatting
    final_prompt = (
        reasoning_start + thoughts + reasoning_end + 
        solution_start + expected_answer + solution_end
    )
    
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": problem},
        {"role": "assistant", "content": final_prompt},
    ]
```

The supervised fine-tuning phase is straightforward - we're essentially teaching the model to copy the format and reasoning style from our training data:

```python
from trl import SFTTrainer, SFTConfig

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset_trim,
    args = SFTConfig(
        per_device_train_batch_size = 1,
        num_train_epochs = 1,  # Usually enough for format learning
        learning_rate = 2e-4,
        optim = "adamw_8bit",
        logging_steps = 5,
    ),
)
trainer.train()
```

This stage is crucial because it gives GRPO a solid starting point. Without it, the model would waste time learning basic formatting instead of focusing on reasoning quality.

## Stage 2: The GRPO Magic

Now comes the exciting part - using GRPO to actually make the model good at reasoning. Unlike traditional training where we just show the model correct examples, GRPO generates multiple attempts and learns from comparing them.

The secret sauce is in the reward functions. We use five different rewards that work together:

### 1. Correctness Reward (The Big One)
```python
def correctness_reward_func(prompts, completions, answer, **kwargs):
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]
```

This gives the biggest reward (2.0) for getting the right answer. Everything else is secondary to correctness.

### 2. Format Rewards (Structure Matters)
```python
def soft_format_reward_func(completions, **kwargs):
    pattern = r"<reasoning>.*?</reasoning>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]
```

These reward functions ensure the model maintains our XML structure, even when it gets creative with its reasoning.

### 3. Incremental Structure Rewards (Baby Steps)
```python
def count_xml(text: str) -> float:
    count = 0.0
    if text.count("<reasoning>\n") == 1: count += 0.125
    if text.count("\n</reasoning>\n") == 1: count += 0.125
    if text.count("\n<answer>\n") == 1: count += 0.125
    if text.count("\n</answer>") == 1: count += 0.125
    return count
```

This function gives partial credit for getting pieces of the format right. It's like scaffolding - helping the model build good structure habits gradually.

## Putting It All Together

The GRPO training configuration brings everything together:

```python
from trl import GRPOTrainer, GRPOConfig

training_args = GRPOConfig(
    temperature = 1.0,
    learning_rate = 5e-6,  # Much lower than SFT
    num_generations = 4,  # Generate multiple attempts
    max_steps = 100,
    optim = "adamw_8bit",
    output_dir = "outputs_gsm8k",
)

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,  
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func  # Most important goes last
    ],
    args = training_args,
    train_dataset = dataset,
)

trainer.train()
```

The beauty of this setup is that the model generates multiple responses to each problem, then learns from the rewards to understand what makes reasoning better. It's like having a teacher who can give nuanced feedback on different aspects of problem-solving.

## What Makes This Special

This approach creates models that don't just give you answers - they show you exactly how they arrived at those answers. The structured format makes it easy to:

- **Verify the logic**: You can follow each step of reasoning
- **Debug mistakes**: When something goes wrong, you can see where
- **Build trust**: Transparent reasoning builds confidence in AI systems
- **Learn from the model**: The reasoning traces are educational

The multi-reward system is particularly clever because it balances different objectives. A model might learn to be very precise with formatting but terrible at math, or great at math but sloppy with structure. By optimizing for both simultaneously, we get models that excel across the board.

## Real-World Impact

I've seen this approach work incredibly well in practice. The models produced feel fundamentally different from traditional language models - they're more like having a mathematical tutor who shows their work rather than a black box that spits out answers.

The applications are endless:
- **Educational tools**: Students can learn by following the model's reasoning
- **Scientific analysis**: Researchers can verify and build upon AI reasoning
- **Business applications**: Decision-makers can understand AI recommendations
- **Code generation**: Models can explain their programming logic

## Key Takeaways

If you're thinking about implementing something similar, here are the crucial insights:

1. **Two-stage training works**: Teaching format first, then optimizing quality
2. **Multiple rewards are powerful**: Different objectives need different signals
3. **Structure enables learning**: The XML format isn't just cosmetic - it shapes how the model thinks
4. **Transparency builds trust**: Showing work makes AI more reliable and verifiable

The code I've shared gives you everything you need to replicate this approach. Start with the supervised fine-tuning to get the format right, then use GRPO to make it actually good at reasoning. The reward functions can be adapted for different domains - just change what you're optimizing for.

## Looking Ahead

This approach represents a shift toward more transparent, trustworthy AI systems. Instead of black boxes that we hope are correct, we're building models that show their work and can be verified by humans.

The next frontiers include applying this to other domains (scientific reasoning, legal analysis, code generation), scaling to larger models, and incorporating human feedback into the reward system. But the foundation is solid - we now have a proven path to building AI that thinks out loud.

The future of AI isn't just about getting the right answers - it's about getting them in ways we can understand, verify, and trust. This approach gets us there.