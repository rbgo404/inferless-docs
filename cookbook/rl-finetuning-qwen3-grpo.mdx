---
title: "Building a Reasoning Model: Fine-tuning Qwen3-4B with GRPO"
description: "Welcome to an engaging tutorial designed to walk you through creating a customer support voicebot where users can voice their queries and receive solutions. You'll learn to integrate speech recognition, large language, and text-to-speech models to develop a responsive and efficient voice-based customer support application."
---

## Introduction

Ever wondered how to make a language model actually "think" through problems step by step? Today, we're diving into a fascinating approach that transforms Qwen3-4B-Base into a reasoning powerhouse using Group Relative Policy Optimization (GRPO).

Unlike traditional fine-tuning where models learn to mimic patterns, this method teaches models to show their work - just like how you'd solve a math problem on paper. We'll walk through the entire process, from setting up the initial model to training it with multiple reward signals that optimize for both correctness and clear reasoning.

The magic happens in two stages: first, we teach the model our reasoning format through supervised fine-tuning, then we use GRPO to make it actually good at reasoning by generating multiple attempts and learning from what works best.

In this blog post, we'll explore how to transform Qwen3-4B-Base into a powerful reasoning model using Group Relative Policy Optimization (GRPO) and OpenR1's Math dataset.

## Why This Approach Works

Traditional language models often feel like black boxes - they give you answers but you can't see the thinking process. This approach fixes that by explicitly structuring the output into reasoning and answer sections. Think of it like teaching a student to show their work in math class.

GRPO takes this further by using multiple reward functions simultaneously. Instead of just caring about the final answer, we reward the model for:
- Getting the right answer (most important)
- Following the proper format
- Using clear reasoning steps
- Maintaining consistent structure

This multi-objective approach creates models that are both accurate and transparent - a game-changer for applications where you need to trust and verify AI reasoning.

## Setting Up the Foundation

We start with Qwen3-4B-Base, a solid general-purpose model that we'll specialize for reasoning. The key is using LoRA (Low-Rank Adaptation) to efficiently fine-tune without breaking the bank on compute resources.

```python
from unsloth import FastLanguageModel
import torch

max_seq_length = 4096  # Room for detailed reasoning
lora_rank = 32  # Sweet spot for capability vs speed

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen3-4B-Base",
    max_seq_length = max_seq_length,
    load_in_4bit = False,  # 16-bit precision for math accuracy
    fast_inference = True,
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.7,
)
```

The extended sequence length gives our model room to breathe when working through complex problems. We're using 16-bit precision instead of 4-bit because mathematical reasoning benefits from the extra precision.

## Creating the Reasoning Template

Here's where we define how we want our model to think. We use a simple but effective XML-like structure:

```python
reasoning_start = "<reasoning>"
reasoning_end = "</reasoning>"
solution_start = "<answer>"
solution_end = "</answer>"

system_prompt = f"""You are given a problem.
Think about the problem and provide your reasoning.
Place it between {reasoning_start} and {reasoning_end}.
Then, provide your solution between:
{solution_start}
...
{solution_end}"""
```

This template forces the model to separate its thinking process from its final answer. It's like requiring students to show their work - the format itself encourages better reasoning habits.

## Stage 1: Teaching the Format

Before we can optimize reasoning quality, we need to teach the model our expected format. This is where supervised fine-tuning comes in - we show the model lots of examples of properly formatted reasoning.

We start with OpenR1's GSM8K dataset, which contains high-quality mathematical reasoning traces. The preprocessing step converts these from DeepSeek's format to our custom structure:

```python
def format_dataset(x):
    expected_answer = x["answer"]
    problem = x["question"]
    
    # Clean up the existing reasoning trace
    thoughts = x["response"].replace("<think>", "").replace("</think>", "").strip()
    
    # Apply our formatting
    final_prompt = (
        reasoning_start + thoughts + reasoning_end + 
        solution_start + expected_answer + solution_end
    )
    
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": problem},
        {"role": "assistant", "content": final_prompt},
    ]
```

The supervised fine-tuning phase is straightforward - we're essentially teaching the model to copy the format and reasoning style from our training data:

```python
from trl import SFTTrainer, SFTConfig

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset_trim,
    args = SFTConfig(
        per_device_train_batch_size = 1,
        num_train_epochs = 1,  # Usually enough for format learning
        learning_rate = 2e-4,
        optim = "adamw_8bit",
        logging_steps = 5,
    ),
)
trainer.train()
```

This stage is crucial because it gives GRPO a solid starting point. Without it, the model would waste time learning basic formatting instead of focusing on reasoning quality.

## Stage 2: GRPO Training with Multi-Objective Rewards

Now comes the exciting part, using GRPO to actually make the model good at reasoning. For this stage, we switch to the standard GSM8K dataset from OpenAI, which provides a clean benchmark for mathematical reasoning problems.

Unlike the initial training phase where we used pre-processed reasoning traces, GRPO training uses the original GSM8K dataset (`openai/gsm8k`) containing math word problems with their expected numerical answers. This dataset is perfect for GRPO because it allows the model to generate its own reasoning paths while being evaluated against known correct answers.

The dataset preprocessing for GRPO is simpler but more focused:

```python
from datasets import load_dataset

dataset = load_dataset('openai/gsm8k', "main", split = "train")

dataset = dataset.map(lambda x: {
    "prompt" : [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": x["question"]},
    ],
    "answer": extract_hash_answer(x["answer"]),  # Extract numerical answer
})
```

Unlike traditional training where we just show the model correct examples, GRPO generates multiple attempts and learns from comparing them. The secret sauce is in the reward functions that work together to optimize different aspects of reasoning:

### The Multi-Reward System

The beauty of GRPO lies in its ability to optimize multiple objectives simultaneously. We use five different reward functions that each focus on a specific aspect of good reasoning:

**1. Correctness Reward (Primary Objective)**
```python
def correctness_reward_func(prompts, completions, answer, **kwargs):
    responses = [completion[0]['content'] for completion in completions]
    extracted_responses = [extract_xml_answer(r) for r in responses]
    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]
```
This gives the biggest reward (2.0) for getting the right answer. Everything else is secondary to correctness.

**2. Format Validation Rewards**
```python
def soft_format_reward_func(completions, **kwargs):
    pattern = r"<reasoning>.*?</reasoning>\s*<answer>.*?</answer>"
    responses = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, r) for r in responses]
    return [0.5 if match else 0.0 for match in matches]
```
These ensure the model maintains our XML structure while allowing for some flexibility in formatting.

**3. Incremental Structure Building**
```python
def count_xml(text: str) -> float:
    count = 0.0
    if text.count("<reasoning>\n") == 1: count += 0.125
    if text.count("\n</reasoning>\n") == 1: count += 0.125
    if text.count("\n<answer>\n") == 1: count += 0.125
    if text.count("\n</answer>") == 1: count += 0.125
    return count
```
This provides partial credit for getting structural elements right, acting as scaffolding for proper format development.

## GRPO Configuration and Training Dynamics

The GRPO training setup brings together all these components in a sophisticated optimization framework:

```python
from trl import GRPOTrainer, GRPOConfig

training_args = GRPOConfig(
    temperature = 1.0,  # Encourages exploration during generation
    learning_rate = 5e-6,  # Much lower than SFT to fine-tune policy
    num_generations = 4,  # Generate multiple attempts per problem
    max_steps = 100,
    optim = "adamw_8bit",
    gradient_accumulation_steps = 1,  # Can increase for smoother training
    per_device_train_batch_size = 1,
    output_dir = "outputs_gsm8k",
)

trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,  
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func  # Most important signal
    ],
    args = training_args,
    train_dataset = dataset,
)

trainer.train()
```

The key insight here is that GRPO generates multiple responses to each problem (`num_generations = 4`) and uses the combined reward signals to understand what constitutes good reasoning. The model learns to associate higher rewards with better reasoning patterns, gradually improving both accuracy and reasoning quality.

The temperature setting of 1.0 is crucial - it provides enough randomness to explore different reasoning approaches while still being guided by the learned patterns. The low learning rate (5e-6) ensures we don't disrupt the base model's capabilities while fine-tuning the reasoning behavior.

## Training Dynamics and Convergence

During GRPO training, fascinating dynamics emerge. Initially, the model might generate responses that get high format rewards but wrong answers, or correct answers with poor reasoning structure. Over time, the multi-objective optimization finds the sweet spot where both correctness and format quality are maximized.

The reward functions work synergistically - the correctness reward ensures the model doesn't just focus on pretty formatting, while the format rewards prevent the model from giving correct answers through poor reasoning. The incremental rewards (like `xmlcount_reward_func`) provide learning signals even when the model hasn't quite mastered the full format yet.

One particularly interesting aspect is how the model learns to self-correct during reasoning. You'll often see the final models generate reasoning traces where they consider multiple approaches, catch their own mistakes, and arrive at the correct answer through clear logical steps.

The `num_generations` parameter is critical for this learning process. By generating multiple attempts, the model can compare different reasoning strategies and learn which ones lead to better outcomes. This is fundamentally different from supervised learning, where the model only sees one "correct" path.

## Performance Monitoring and Evaluation

Throughout training, you can monitor the model's progress by observing how the different reward components evolve:

- **Correctness rewards** should steadily increase as mathematical accuracy improves
- **Format rewards** typically improve quickly early in training as the model learns structure
- **The balance between rewards** indicates whether the model is trading off accuracy for formatting or vice versa

The GRPO framework provides detailed logging of these reward components, allowing you to understand exactly how your model is learning and whether any adjustments to the reward weighting might be beneficial.

## Conclusion

This GRPO-based approach to building reasoning models represents a significant advance in AI transparency and reliability. By combining supervised format learning with multi-objective reinforcement learning, we create models that don't just provide answers but demonstrate clear, verifiable reasoning processes.

The two-stage approach - first teaching structure through supervised fine-tuning, then optimizing reasoning quality through GRPO - proves to be highly effective. The use of multiple reward functions ensures that we optimize for both correctness and interpretability simultaneously, creating models that are both accurate and trustworthy.

What makes this approach particularly powerful is its generalizability. While we've focused on mathematical reasoning using GSM8K, the same framework can be adapted to other domains by adjusting the reward functions and training data. Whether you're working on scientific reasoning, code generation, or logical analysis, the principles of structured reasoning and multi-objective optimization remain applicable.

The resulting models feel fundamentally different from traditional language models - they're more like having an expert assistant who shows their work rather than a black box that produces answers. This transparency is crucial for applications where trust and verification matter, from educational tools to scientific research to business decision-making.

As AI systems become more capable and more widely deployed, approaches like this that prioritize interpretability alongside performance will become increasingly important. The ability to peer into an AI's reasoning process, understand its logic, and verify its conclusions represents a crucial step toward more responsible and trustworthy artificial intelligence.